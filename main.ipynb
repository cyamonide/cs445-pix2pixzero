{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyamonide/cs445-pix2pixzero/blob/siliu%2Fedit-directions/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEZ_qmqnfBCR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/pix2pix-zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-PEl7Jo9NE6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNsNoPDN9NE9"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install tensorflow[and-cuda]\n",
        "!pip install tensorrt accelerate diffusers==0.12.0 einops gradio ipython numpy opencv-python-headless pillow psutil tqdm transformers\n",
        "!pip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install salesforce-lavis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY55XotC_fDS"
      },
      "source": [
        "## **Method**\n",
        "Overview: First, text captions are generated from an input image via BLIP, followed by the creation of an inverse noise map through regularized DDIM inversion. Reference cross-attention maps are then produced to match the image structure, guided by CLIP embeddings of the generated text. Denoising with edited text embeddings ensues, with a focus on aligning current cross-attention maps with the reference ones.\n",
        "<br>\n",
        "<br>\n",
        "<img src=\"https://pix2pixzero.github.io/assets/method_cat.jpg\" width=1500 />\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lsQXgYv_hmy"
      },
      "source": [
        "### **1. Inverting Real Images**\n",
        "The paper adopts the deterministic DDIM reverse process fro inverting real images into their latent representations. The key steps include:\n",
        "\n",
        "\n",
        "*   Preparation of the input image and latents\n",
        "*   Implementation of deterministic DDIM reverse process ([Denoising Diffusion Implicit Models](https://arxiv.org/pdf/2010.02502.pdf))\n",
        "<br> Inversion entails finding a noise map $x_{inv}$ that reconstructs the input latent code $x_0$ upon sampling. The deterministic DDIM reverse process shows below:\n",
        "$$ x_{t+1} = \\sqrt{\\alpha_{t+1}^{-}}f_{\\theta}(x_t, t, c) + \\sqrt{1-\\alpha_{t+1}^{-}}\\epsilon_{\\theta}(x_t, t, c) $$\n",
        "where $x_t$ is noised latent code at timestep t, $\\epsilon_\\theta (x_t , t, c)$ is a UNet-based denoiser that predicts added noise in $x_t$ conditional on timestep t and encoded text features c, $\\alpha_{t+1}^{-}$ is noise scaling factor as defined in DDIM, and $f_{\\theta}(x_t, t, c)$ predicts the final denoised latent code $x_0$.\n",
        "$$f_{\\theta}(x_t, t, c) = \\frac{x_t - \\sqrt{1-\\alpha_t^{-}}\\epsilon_{\\theta}(x_t, t, c)}{\\sqrt{\\alpha_t^{-}}} $$\n",
        "* Noise regularization\n",
        "<br> - Auto-correlation regularization $L_{pair}$\n",
        "$$\n",
        "L_{pair} = \\sum_{p} \\frac{1}{S_{p}^{2}} \\sum_{\\delta=1}^{S_{p}-1} \\sum_{x,y,c} \\eta^{p}_{x,y,c} (\\eta^{p}_{x-\\delta,y,c} + \\eta^{p}_{x,y-\\delta,c}),\n",
        "$$\n",
        "<br> - KL Divergence regularization $L_{KL}$\n",
        "<br> - Final autocorrelation regularization $L_{auto} = L_{pair} + \\lambda L_{KL}$\n",
        "* Image reconstruction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03TCxovz_l4W"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from random import randrange\n",
        "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
        "from diffusers import DDIMScheduler\n",
        "from diffusers.schedulers.scheduling_ddim import DDIMSchedulerOutput\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "from src.utils.base_pipeline import BasePipeline\n",
        "from src.utils.cross_attention import prep_unet\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "class DDIMInversion(BasePipeline):\n",
        "  def __call__(\n",
        "    self,\n",
        "    prompt: Union[str, List[str]] = None,\n",
        "    num_inversion_steps: int = 50,\n",
        "    guidance_scale: float = 7.5,\n",
        "    negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "    num_images_per_prompt: Optional[int] = 1,\n",
        "    eta: float = 0.0,\n",
        "    output_type: Optional[str] = \"pil\",\n",
        "    return_dict: bool = True,\n",
        "    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    img=None,\n",
        "    torch_dtype=torch.float32,\n",
        "\n",
        "    # inversion regularization parameters\n",
        "    lambda_kl: float = 20.0,\n",
        "    num_reg_steps: int = 5,\n",
        "    num_ac_rolls: int = 5,\n",
        "):\n",
        "    self.unet = prep_unet(self.unet)\n",
        "    device = self._execution_device\n",
        "    do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "    # Prepare the input image and encode to latent space\n",
        "    x0 = self.prep_img(img, device, torch_dtype)\n",
        "    x0_enc = self.encode_latents(x0, device, torch_dtype)\n",
        "\n",
        "    # Decode\n",
        "    with torch.no_grad():\n",
        "      x0_dec = self.decode_latents(x0_enc.detach())\n",
        "    image_x0_dec = self.numpy_to_pil(x0_dec)\n",
        "\n",
        "    # DDIM inversion\n",
        "    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt).to(device)\n",
        "    extra_step_kwargs = self.prepare_extra_step_kwargs(None, eta)\n",
        "\n",
        "    latents = x0_enc\n",
        "    latents = self.perform_inversion(x0_enc, num_inversion_steps, device, torch_dtype, prompt_embeds, extra_step_kwargs, \\\n",
        "    do_classifier_free_guidance, cross_attention_kwargs, guidance_scale, num_reg_steps, lambda_kl, num_ac_rolls, latents)\n",
        "\n",
        "    x_inv = latents.detach().clone()\n",
        "\n",
        "    # Decode the latent back to image space\n",
        "    img = self.decode_latents(latents.detach())\n",
        "    img = self.numpy_to_pil(img)\n",
        "\n",
        "    return x_inv, img, image_x0_dec\n",
        "\n",
        "  # prepare the input image\n",
        "  def prep_img (self, img, device, torch_dtype):\n",
        "    x0 = np.array(img) / 255\n",
        "    x0 = torch.from_numpy(x0).type(torch_dtype).permute(2, 0, 1).unsqueeze(0).repeat(1, 1, 1, 1).to(device)\n",
        "    return (x0 - 0.5) * 2\n",
        "\n",
        "  # encode to latent space\n",
        "  def encode_latents(self, x0, device, torch_dtype):\n",
        "    with torch.no_grad():\n",
        "      # vae: AutoencoderKL\n",
        "      x0_enc = self.vae.encode(x0).latent_dist.sample().to(device, torch_dtype)\n",
        "    # normalization\n",
        "    return 0.18215 * x0_enc\n",
        "\n",
        "  # perform the DDIM inversion\n",
        "  def perform_inversion(self, x0_enc, num_inversion_steps, device, torch_dtype, prompt_embeds, \\\n",
        "  extra_step_kwargs, do_classifier_free_guidance, cross_attention_kwargs, guidance_scale, \\\n",
        "  num_reg_steps, lambda_kl, num_ac_rolls, latents):\n",
        "    # set timesteps\n",
        "    self.scheduler.set_timesteps(num_inversion_steps, device=device)\n",
        "    timesteps = self.scheduler.timesteps\n",
        "\n",
        "    num_warmup_steps = len(timesteps) - num_inversion_steps * self.scheduler.order\n",
        "    with self.progress_bar(total=num_inversion_steps) as progress_bar:\n",
        "      for i, t in enumerate(timesteps.flip(0)[1:-1]):\n",
        "        # implementation of classifier-free guidance\n",
        "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "        # alpha_t: adjusts the input latents according to a noise schedule\n",
        "        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "        # e_t: a U-Net model (f_theta) predicts the noise for the given timesteps\n",
        "        with torch.no_grad():\n",
        "            noise_pred = self.unet(latent_model_input,t,encoder_hidden_states=prompt_embeds,cross_attention_kwargs=cross_attention_kwargs,).sample\n",
        "        # print(f\"noise_pred before regularization: {noise_pred} \\n\")\n",
        "\n",
        "        noise_pred = self.apply_regularization(noise_pred, lambda_kl, do_classifier_free_guidance, \\\n",
        "        guidance_scale, num_reg_steps, num_ac_rolls)\n",
        "        # print(f\"noise_pred after regularization: {noise_pred} \\n\")\n",
        "\n",
        "        # x_t steps back towards x_{t-1}: updates the latents by removing the noise epsilon_{\\theta}\n",
        "        latents = self.scheduler.step(noise_pred, t, latents, reverse=True, **extra_step_kwargs).prev_sample\n",
        "\n",
        "        # callback\n",
        "        if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "            progress_bar.update()\n",
        "\n",
        "    return latents\n",
        "\n",
        "  # apply auto-correclation and KL divergence regularization\n",
        "  def apply_regularization(self, noise_pred, lambda_kl, do_classifier_free_guidance, guidance_scale, num_reg_steps, num_ac_rolls):\n",
        "    # perform guidance\n",
        "    if do_classifier_free_guidance:\n",
        "        # split the noise predcition tensor into two parts: one unconditioned and one conditioned on text\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # regularization of the noise prediction\n",
        "    e_t = noise_pred\n",
        "    for _ in range(num_reg_steps):\n",
        "      e_t = self.regularize_noise(e_t,lambda_kl, num_ac_rolls)\n",
        "    return e_t\n",
        "\n",
        "  def regularize_noise(self, e_t, lambda_kl, num_ac_rolls):\n",
        "    for _ in range(num_ac_rolls):\n",
        "      e_t = self.apply_auto_corr_loss(e_t)\n",
        "      if lambda_kl > 0:\n",
        "          e_t = self.apply_kl_divergence_loss(e_t, lambda_kl)\n",
        "    return e_t\n",
        "\n",
        "  # def apply_auto_corr_loss(self, e_t, lambda_ac):\n",
        "  #   var = e_t.detach().clone().requires_grad_(True)\n",
        "  #   # compute the auto_corr_loss\n",
        "  #   l_ac = self.auto_corr_loss(var)\n",
        "  #   # compute the gradients and store the gradients in var.grad\n",
        "  #   l_ac.backward()\n",
        "  #   grad = var.grad.detach()\n",
        "\n",
        "  #   return e_t - lambda_ac * grad\n",
        "\n",
        "  def apply_auto_corr_loss(self, e_t):\n",
        "    e_t.requires_grad_(True)\n",
        "    # use SGD optimizer\n",
        "    optimizer = torch.optim.SGD([e_t], lr=0.1)\n",
        "    optimizer.zero_grad()\n",
        "    # calculate the auto_corr_loss\n",
        "    l_ac = self.auto_corr_loss(e_t)\n",
        "    # calculate gradients\n",
        "    l_ac.backward()\n",
        "    optimizer.step()\n",
        "    e_t.detach_()\n",
        "\n",
        "    return e_t\n",
        "\n",
        "  # L_pair\n",
        "  def auto_corr_loss(self, x, random_shift=True):\n",
        "    B, C, H, W = x.shape\n",
        "    assert B == 1\n",
        "    # [C, H, W]\n",
        "    x = x.squeeze(0)\n",
        "\n",
        "    reg_loss = 0.0\n",
        "    for channel in x:\n",
        "      # [1, 1, H, W]\n",
        "      noise = channel.unsqueeze(0).unsqueeze(0)\n",
        "      # H == W\n",
        "      current_size = noise.size(2)\n",
        "\n",
        "      while current_size >= 8:\n",
        "        if random_shift and current_size > 1:\n",
        "          roll_amount_h = randrange(current_size // 2)\n",
        "          roll_amount_w = randrange(current_size // 2)\n",
        "        else:\n",
        "          roll_amount_h, roll_amount_w = 1, 1\n",
        "\n",
        "        # Compute autocorrelation for shifted tensors in both dimensions\n",
        "        rolled_h = torch.roll(noise, shifts=roll_amount_h, dims=2)\n",
        "        rolled_w = torch.roll(noise, shifts=roll_amount_w, dims=3)\n",
        "        reg_loss += (noise * rolled_h).mean().pow(2)\n",
        "        reg_loss += (noise * rolled_w).mean().pow(2)\n",
        "\n",
        "        # Reduce size by pooling\n",
        "        noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "        current_size = noise.size(2)\n",
        "\n",
        "    return reg_loss\n",
        "\n",
        "  def apply_kl_divergence_loss(self, e_t, lambda_kl):\n",
        "    var = e_t.detach().clone().requires_grad_(True)\n",
        "    # compute the auto_corr_loss\n",
        "    l_kld = self.kl_divergence(var)\n",
        "    # compute the gradients and store the gradients in var.grad\n",
        "    l_kld.backward()\n",
        "    grad = var.grad.detach()\n",
        "    return e_t - lambda_kl * grad\n",
        "\n",
        "  # calculate the KL divergence for regularization\n",
        "  def kl_divergence(self, x):\n",
        "    _mu = x.mean()\n",
        "    _var = x.var()\n",
        "\n",
        "    # KL divergence for Gaussian distribution with mean=0 and variance=1\n",
        "    return _var + _mu**2 - 1 - torch.log(_var + 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn0n-7oX_t35"
      },
      "source": [
        "### **2. Discovering Edit Directions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUMxTGQPfofF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "\"\"\"\n",
        "Remove any undesirable characters that might affect edit direction generation.\n",
        "\"\"\"\n",
        "def clean_sentences(sentences):\n",
        "  characters_to_remove = \"012345679.,'-():!\"\n",
        "  cleaned_sentences = sentences\n",
        "\n",
        "  for c in characters_to_remove:\n",
        "    cleaned_sentences = [s.replace(c, \"\") for s in cleaned_sentences]\n",
        "\n",
        "  cleaned_sentences = [s.strip().lower() for s in cleaned_sentences]\n",
        "\n",
        "  return cleaned_sentences\n",
        "\n",
        "\"\"\"\n",
        "Use Google's Flan T5 XL model to generate :num: sentences containing a\n",
        "particular word. By default, 1024 sentences are generated since ~1000 is\n",
        "recommended for reasonable results:\n",
        "https://github.com/pix2pixzero/pix2pix-zero/issues/16#issuecomment-1485394203\n",
        "\"\"\"\n",
        "def compute_sentences(word, count=1024):\n",
        "  model_name = \"google/flan-t5-xl\"\n",
        "  prompt = f\"Provide a caption for images containing {word}. The captions should be in English and be shorter than 150 characters. The caption must contain the word {word}.\"\n",
        "\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "  prompt_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "  prompt_length = prompt_ids.shape[1]\n",
        "\n",
        "  sentences = []\n",
        "\n",
        "  while len(sentences) < count:\n",
        "    try:\n",
        "      output_ids = model.generate(\n",
        "          prompt_ids,\n",
        "          do_sample=True,\n",
        "          num_return_sequences=8,\n",
        "          max_length=128,\n",
        "          min_length=15\n",
        "      )\n",
        "      output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    sentences += output\n",
        "\n",
        "    print(f\"{len(sentences)}/{count} generated.\")\n",
        "\n",
        "  # clean\n",
        "  sentences = clean_sentences(sentences)\n",
        "\n",
        "  del model\n",
        "  del tokenizer\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vW1i1MfgN1-"
      },
      "outputs": [],
      "source": [
        "word = \"horse\"\n",
        "sentences = compute_sentences(word, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MTh5u7enVx3"
      },
      "outputs": [],
      "source": [
        "with open(f\"sentences/{word}.txt\", 'w') as f:\n",
        "  for s in sentences:\n",
        "    f.write(f\"{s}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "from src.utils.edit_pipeline import EditingPipeline\n",
        "\n",
        "def get_edit_direction(task_name):\n",
        "  task_from = task_name.split('2')[0]\n",
        "  task_to = task_name.split('2')[1]\n",
        "\n",
        "  model_name = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "  pipe = EditingPipeline.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
        "\n",
        "  tokenizer = pipe.tokenizer\n",
        "  encoder = pipe.text_encoder\n",
        "\n",
        "  # tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
        "  # encoder = CLIPTextModel.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "  # Create embeddings\n",
        "  for thing in task_name.split('2'):\n",
        "    sentence_filepath = f\"sentences/{thing}.txt\"\n",
        "    embedding_filepath = f\"assets/embeddings_sd_1.4/{thing}.pt\"\n",
        "\n",
        "    # Skip embedding generation if it already exists\n",
        "    if os.path.exists(embedding_filepath):\n",
        "      print(f\"{embedding_filepath} already exists, skipping generation!\")\n",
        "      continue\n",
        "\n",
        "    with open(sentence_filepath, 'r') as f:\n",
        "      sentences = [l.strip() for l in f.readlines()]\n",
        "\n",
        "    sentence_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for s in sentences:\n",
        "        sentence_t = tokenizer(\n",
        "            s,\n",
        "            padding=\"max_length\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        sentence_ids = sentence_t.input_ids\n",
        "        embeds = encoder(sentence_ids.to(device), attention_mask=None)[0]\n",
        "        sentence_embeddings.append(embeds)\n",
        "\n",
        "        # memory mgmt\n",
        "        del sentence_ids\n",
        "\n",
        "      average_embeddings = torch.concatenate(sentence_embeddings, dim=0).mean(dim=0).unsqueeze(0)\n",
        "\n",
        "      print(f\"Saving embeddings file {embedding_filepath}...\")\n",
        "      torch.save(average_embeddings, embedding_filepath)\n",
        "\n",
        "  del tokenizer\n",
        "  del encoder\n",
        "  del pipe\n",
        "\n",
        "  # Compute difference between embeddings\n",
        "  from_embeddings = torch.load(f\"assets/embeddings_sd_1.4/{task_from}.pt\", map_location=device)\n",
        "  to_embeddings = torch.load(f\"assets/embeddings_sd_1.4/{task_to}.pt\", map_location=device)\n",
        "\n",
        "  difference = (to_embeddings.mean(0) - from_embeddings.mean(0)).unsqueeze(0)\n",
        "\n",
        "  return difference"
      ],
      "metadata": {
        "id": "9ewaFNZpI1Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_edit_direction(\"car2horse\")"
      ],
      "metadata": {
        "id": "rXpb_R9SNJi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python src/edit_synthetic.py \\\n",
        "    --results_folder \"output/synth_editing_car\" \\\n",
        "    --prompt_str \"a high resolution photo of the side of a car\" \\\n",
        "    --task \"car2horse\"\\\n",
        "    --use_float_16"
      ],
      "metadata": {
        "id": "lz-mjF4_OI39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "break"
      ],
      "metadata": {
        "id": "shuCAjr5OJtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqNtQOnLofsA"
      },
      "outputs": [],
      "source": [
        "! python src/make_edit_direction.py \\\n",
        "        --file_source_sentences sentences/pineapple.txt \\\n",
        "        --file_target_sentences sentences/strawberry.txt \\\n",
        "        --output_folder assets/embeddings_sd_1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nOtozKsZrJog"
      },
      "outputs": [],
      "source": [
        "! python src/inversion.py  \\\n",
        "        --input_image \"assets/test_images/pineapples/pineapple_1.jpg\" \\\n",
        "        --results_folder \"output/test_pineapple_3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75xj68Y0rTVg"
      },
      "outputs": [],
      "source": [
        "! python src/edit_real.py \\\n",
        "    --inversion \"output/test_pineapple_3/inversion/pineapple_1.pt\" \\\n",
        "    --prompt \"output/test_pineapple_3/prompt/pineapple_1.txt\" \\\n",
        "    --task_name \"pineapple2strawberry\" \\\n",
        "    --results_folder \"output/test_pineapple_3/\"\\\n",
        "    --use_float_16 \\\n",
        "    --edit_direction_multiplier 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twEhCN8g_vdo"
      },
      "source": [
        "### **3. Editing via Cross-Attention Guidance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WsoS3XF_4Fg"
      },
      "source": [
        "## **Running the code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXSr6lPVAPvc"
      },
      "outputs": [],
      "source": [
        "! python src/inversion.py  \\\n",
        "        --input_image \"assets/test_images/cats/cat_7.png\" \\\n",
        "        --results_folder \"output/test_cat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4trPHhdqAQjO"
      },
      "outputs": [],
      "source": [
        "! python src/edit_real.py \\\n",
        "    --inversion \"output/test_cat/inversion/cat_7.pt\" \\\n",
        "    --prompt \"output/test_cat/prompt/cat_7.txt\" \\\n",
        "    --task_name \"cat2dog\" \\\n",
        "    --results_folder \"output/test_cat/\"\\\n",
        "    --use_float_16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2NCn5p_ATCj"
      },
      "outputs": [],
      "source": [
        "! python src/edit_synthetic.py \\\n",
        "    --results_folder \"output/synth_editing\" \\\n",
        "    --prompt_str \"a high resolution painting of a cat in the style of van gogh\" \\\n",
        "    --task \"cat2dog\"\\\n",
        "    --use_float_16"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}