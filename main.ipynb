{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-PEl7Jo9NE6",
        "outputId": "ec5568c4-7716-4a8b-c4ec-c5ad5cf41225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.1\n",
            "None\n",
            "Python 3.11.5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNsNoPDN9NE9"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install tensorflow[and-cuda]\n",
        "!pip install tensorrt\n",
        "!pip install accelerate\n",
        "!pip install diffusers==0.12.0\n",
        "!pip install einops\n",
        "!pip install gradio\n",
        "!pip install ipython\n",
        "!pip install numpy\n",
        "!pip install opencv-python-headless\n",
        "!pip install pillow\n",
        "!pip install psutil\n",
        "!pip install tqdm\n",
        "!pip install transformers\n",
        "!pip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install salesforce-lavis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method**\n",
        "Overview: First, text captions are generated from an input image via BLIP, followed by the creation of an inverse noise map through regularized DDIM inversion. Reference cross-attention maps are then produced to match the image structure, guided by CLIP embeddings of the generated text. Denoising with edited text embeddings ensues, with a focus on aligning current cross-attention maps with the reference ones.\n",
        "<br>\n",
        "<br>\n",
        "<img src=\"https://pix2pixzero.github.io/assets/method_cat.jpg\" width=1500 />\n",
        "<br>"
      ],
      "metadata": {
        "id": "wY55XotC_fDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Inverting Real Images**\n",
        "The paper adopts the deterministic DDIM reverse process fro inverting real images into their latent representations. The key steps include:\n",
        "\n",
        "\n",
        "*   Preparation of the input image and latents\n",
        "*   Implementation of deterministic DDIM reverse process ([Denoising Diffusion Implicit Models](https://arxiv.org/pdf/2010.02502.pdf))\n",
        "<br> Inversion entails finding a noise map $x_{inv}$ that reconstructs the input latent code $x_0$ upon sampling. The deterministic DDIM reverse process shows below:\n",
        "$$ x_{t+1} = \\sqrt{\\alpha_{t+1}^{-}}f_{\\theta}(x_t, t, c) + \\sqrt{1-\\alpha_{t+1}^{-}}\\epsilon_{\\theta}(x_t, t, c) $$\n",
        "where $x_t$ is noised latent code at timestep t, $\\epsilon_\\theta (x_t , t, c)$ is a UNet-based denoiser that predicts added noise in $x_t$ conditional on timestep t and encoded text features c, $\\alpha_{t+1}^{-}$ is noise scaling factor as defined in DDIM, and $f_{\\theta}(x_t, t, c)$ predicts the final denoised latent code $x_0$.\n",
        "$$f_{\\theta}(x_t, t, c) = \\frac{x_t - \\sqrt{1-\\alpha_t^{-}}\\epsilon_{\\theta}(x_t, t, c)}{\\sqrt{\\alpha_t^{-}}} $$\n",
        "* Noise regularization\n",
        "<br> - Auto-correlation regularization $L_{pair}$\n",
        "$$\n",
        "L_{pair} = \\sum_{p} \\frac{1}{S_{p}^{2}} \\sum_{\\delta=1}^{S_{p}-1} \\sum_{x,y,c} \\eta^{p}_{x,y,c} (\\eta^{p}_{x-\\delta,y,c} + \\eta^{p}_{x,y-\\delta,c}),\n",
        "$$\n",
        "<br> - KL Divergence regularization $L_{KL}$\n",
        "<br> - Final autocorrelation regularization $L_{auto} = L_{pair} + \\lambda L_{KL}$\n",
        "* Image reconstruction\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0lsQXgYv_hmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from random import randrange\n",
        "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
        "from diffusers import DDIMScheduler\n",
        "from diffusers.schedulers.scheduling_ddim import DDIMSchedulerOutput\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "from src.utils.base_pipeline import BasePipeline\n",
        "from src.utils.cross_attention import prep_unet\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "class DDIMInversion(BasePipeline):\n",
        "  def __call__(\n",
        "    self,\n",
        "    prompt: Union[str, List[str]] = None,\n",
        "    num_inversion_steps: int = 50,\n",
        "    guidance_scale: float = 7.5,\n",
        "    negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "    num_images_per_prompt: Optional[int] = 1,\n",
        "    eta: float = 0.0,\n",
        "    output_type: Optional[str] = \"pil\",\n",
        "    return_dict: bool = True,\n",
        "    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    img=None,\n",
        "    torch_dtype=torch.float32,\n",
        "\n",
        "    # inversion regularization parameters\n",
        "    lambda_kl: float = 20.0,\n",
        "    num_reg_steps: int = 5,\n",
        "    num_ac_rolls: int = 5,\n",
        "):\n",
        "    self.unet = prep_unet(self.unet)\n",
        "    device = self._execution_device\n",
        "    do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "    # Prepare the input image and encode to latent space\n",
        "    x0 = self.prep_img(img, device, torch_dtype)\n",
        "    x0_enc = self.encode_latents(x0, device, torch_dtype)\n",
        "\n",
        "    # Decode\n",
        "    with torch.no_grad():\n",
        "      x0_dec = self.decode_latents(x0_enc.detach())\n",
        "    image_x0_dec = self.numpy_to_pil(x0_dec)\n",
        "\n",
        "    # DDIM inversion\n",
        "    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt).to(device)\n",
        "    extra_step_kwargs = self.prepare_extra_step_kwargs(None, eta)\n",
        "\n",
        "    latents = x0_enc\n",
        "    latents = self.perform_inversion(x0_enc, num_inversion_steps, device, torch_dtype, prompt_embeds, extra_step_kwargs, \\\n",
        "    do_classifier_free_guidance, cross_attention_kwargs, guidance_scale, num_reg_steps, lambda_kl, num_ac_rolls, latents)\n",
        "\n",
        "    x_inv = latents.detach().clone()\n",
        "\n",
        "    # Decode the latent back to image space\n",
        "    img = self.decode_latents(latents.detach())\n",
        "    img = self.numpy_to_pil(img)\n",
        "\n",
        "    return x_inv, img, image_x0_dec\n",
        "\n",
        "  # prepare the input image\n",
        "  def prep_img (self, img, device, torch_dtype):\n",
        "    x0 = np.array(img) / 255\n",
        "    x0 = torch.from_numpy(x0).type(torch_dtype).permute(2, 0, 1).unsqueeze(0).repeat(1, 1, 1, 1).to(device)\n",
        "    return (x0 - 0.5) * 2\n",
        "\n",
        "  # encode to latent space\n",
        "  def encode_latents(self, x0, device, torch_dtype):\n",
        "    with torch.no_grad():\n",
        "      # vae: AutoencoderKL\n",
        "      x0_enc = self.vae.encode(x0).latent_dist.sample().to(device, torch_dtype)\n",
        "    # normalization\n",
        "    return 0.18215 * x0_enc\n",
        "\n",
        "  # perform the DDIM inversion\n",
        "  def perform_inversion(self, x0_enc, num_inversion_steps, device, torch_dtype, prompt_embeds, \\\n",
        "  extra_step_kwargs, do_classifier_free_guidance, cross_attention_kwargs, guidance_scale, \\\n",
        "  num_reg_steps, lambda_kl, num_ac_rolls, latents):\n",
        "    # set timesteps\n",
        "    self.scheduler.set_timesteps(num_inversion_steps, device=device)\n",
        "    timesteps = self.scheduler.timesteps\n",
        "\n",
        "    num_warmup_steps = len(timesteps) - num_inversion_steps * self.scheduler.order\n",
        "    with self.progress_bar(total=num_inversion_steps) as progress_bar:\n",
        "      for i, t in enumerate(timesteps.flip(0)[1:-1]):\n",
        "        # implementation of classifier-free guidance\n",
        "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "        # alpha_t: adjusts the input latents according to a noise schedule\n",
        "        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "        # e_t: a U-Net model (f_theta) predicts the noise for the given timesteps\n",
        "        with torch.no_grad():\n",
        "            noise_pred = self.unet(latent_model_input,t,encoder_hidden_states=prompt_embeds,cross_attention_kwargs=cross_attention_kwargs,).sample\n",
        "        # print(f\"noise_pred before regularization: {noise_pred} \\n\")\n",
        "\n",
        "        noise_pred = self.apply_regularization(noise_pred, lambda_kl, do_classifier_free_guidance, \\\n",
        "        guidance_scale, num_reg_steps, num_ac_rolls)\n",
        "        # print(f\"noise_pred after regularization: {noise_pred} \\n\")\n",
        "\n",
        "        # x_t steps back towards x_{t-1}: updates the latents by removing the noise epsilon_{\\theta}\n",
        "        latents = self.scheduler.step(noise_pred, t, latents, reverse=True, **extra_step_kwargs).prev_sample\n",
        "\n",
        "        # callback\n",
        "        if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "            progress_bar.update()\n",
        "\n",
        "    return latents\n",
        "\n",
        "  # apply auto-correclation and KL divergence regularization\n",
        "  def apply_regularization(self, noise_pred, lambda_kl, do_classifier_free_guidance, guidance_scale, num_reg_steps, num_ac_rolls):\n",
        "    # perform guidance\n",
        "    if do_classifier_free_guidance:\n",
        "        # split the noise predcition tensor into two parts: one unconditioned and one conditioned on text\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # regularization of the noise prediction\n",
        "    e_t = noise_pred\n",
        "    for _ in range(num_reg_steps):\n",
        "      e_t = self.regularize_noise(e_t,lambda_kl, num_ac_rolls)\n",
        "    return e_t\n",
        "\n",
        "  def regularize_noise(self, e_t, lambda_kl, num_ac_rolls):\n",
        "    for _ in range(num_ac_rolls):\n",
        "      e_t = self.apply_auto_corr_loss(e_t)\n",
        "      if lambda_kl > 0:\n",
        "          e_t = self.apply_kl_divergence_loss(e_t, lambda_kl)\n",
        "    return e_t\n",
        "\n",
        "  # def apply_auto_corr_loss(self, e_t, lambda_ac):\n",
        "  #   var = e_t.detach().clone().requires_grad_(True)\n",
        "  #   # compute the auto_corr_loss\n",
        "  #   l_ac = self.auto_corr_loss(var)\n",
        "  #   # compute the gradients and store the gradients in var.grad\n",
        "  #   l_ac.backward()\n",
        "  #   grad = var.grad.detach()\n",
        "\n",
        "  #   return e_t - lambda_ac * grad\n",
        "\n",
        "  def apply_auto_corr_loss(self, e_t):\n",
        "    e_t.requires_grad_(True)\n",
        "    # use SGD optimizer\n",
        "    optimizer = torch.optim.SGD([e_t], lr=0.1)\n",
        "    optimizer.zero_grad()\n",
        "    # calculate the auto_corr_loss\n",
        "    l_ac = self.auto_corr_loss(e_t)\n",
        "    # calculate gradients\n",
        "    l_ac.backward()\n",
        "    optimizer.step()\n",
        "    e_t.detach_()\n",
        "\n",
        "    return e_t\n",
        "\n",
        "  # L_pair\n",
        "  def auto_corr_loss(self, x, random_shift=True):\n",
        "    B, C, H, W = x.shape\n",
        "    assert B == 1\n",
        "    # [C, H, W]\n",
        "    x = x.squeeze(0)\n",
        "\n",
        "    reg_loss = 0.0\n",
        "    for channel in x:\n",
        "      # [1, 1, H, W]\n",
        "      noise = channel.unsqueeze(0).unsqueeze(0)\n",
        "      # H == W\n",
        "      current_size = noise.size(2)\n",
        "\n",
        "      while current_size >= 8:\n",
        "        if random_shift and current_size > 1:\n",
        "          roll_amount_h = randrange(current_size // 2)\n",
        "          roll_amount_w = randrange(current_size // 2)\n",
        "        else:\n",
        "          roll_amount_h, roll_amount_w = 1, 1\n",
        "\n",
        "        # Compute autocorrelation for shifted tensors in both dimensions\n",
        "        rolled_h = torch.roll(noise, shifts=roll_amount_h, dims=2)\n",
        "        rolled_w = torch.roll(noise, shifts=roll_amount_w, dims=3)\n",
        "        reg_loss += (noise * rolled_h).mean().pow(2)\n",
        "        reg_loss += (noise * rolled_w).mean().pow(2)\n",
        "\n",
        "        # Reduce size by pooling\n",
        "        noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "        current_size = noise.size(2)\n",
        "\n",
        "    return reg_loss\n",
        "\n",
        "  def apply_kl_divergence_loss(self, e_t, lambda_kl):\n",
        "    var = e_t.detach().clone().requires_grad_(True)\n",
        "    # compute the auto_corr_loss\n",
        "    l_kld = self.kl_divergence(var)\n",
        "    # compute the gradients and store the gradients in var.grad\n",
        "    l_kld.backward()\n",
        "    grad = var.grad.detach()\n",
        "    return e_t - lambda_kl * grad\n",
        "\n",
        "  # calculate the KL divergence for regularization\n",
        "  def kl_divergence(self, x):\n",
        "    _mu = x.mean()\n",
        "    _var = x.var()\n",
        "\n",
        "    # KL divergence for Gaussian distribution with mean=0 and variance=1\n",
        "    return _var + _mu**2 - 1 - torch.log(_var + 1e-7)"
      ],
      "metadata": {
        "id": "03TCxovz_l4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Discovering Edit Directions**"
      ],
      "metadata": {
        "id": "Gn0n-7oX_t35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Editing via Cross-Attention Guidance**"
      ],
      "metadata": {
        "id": "twEhCN8g_vdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Running the code**"
      ],
      "metadata": {
        "id": "_WsoS3XF_4Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/pix2pix-zero"
      ],
      "metadata": {
        "id": "ynzER7hL_1_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python src/inversion.py  \\\n",
        "        --input_image \"assets/test_images/cats/cat_7.png\" \\\n",
        "        --results_folder \"output/test_cat\""
      ],
      "metadata": {
        "id": "VXSr6lPVAPvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python src/edit_real.py \\\n",
        "    --inversion \"output/test_cat/inversion/cat_7.pt\" \\\n",
        "    --prompt \"output/test_cat/prompt/cat_7.txt\" \\\n",
        "    --task_name \"cat2dog\" \\\n",
        "    --results_folder \"output/test_cat/\"\\\n",
        "    --use_float_16"
      ],
      "metadata": {
        "id": "4trPHhdqAQjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python src/edit_synthetic.py \\\n",
        "    --results_folder \"output/synth_editing\" \\\n",
        "    --prompt_str \"a high resolution painting of a cat in the style of van gogh\" \\\n",
        "    --task \"cat2dog\"\\\n",
        "    --use_float_16"
      ],
      "metadata": {
        "id": "k2NCn5p_ATCj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}